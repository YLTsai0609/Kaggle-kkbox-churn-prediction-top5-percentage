{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import table\n",
    "import datetime as dt \n",
    "# dt as time_series operation\n",
    "%matplotlib inline\n",
    "# for display dataframe\n",
    "from IPython.display import display\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "# ignore warning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# storage\n",
    "from google.datalab import Context\n",
    "import google.datalab.bigquery as bq\n",
    "try:\n",
    "  from StringIO import StringIO\n",
    "except ImportError:\n",
    "  from io import BytesIO as StringIO\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "# square scale\n",
    "import matplotlib.scale as mscale\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "class SquareRootScale(mscale.ScaleBase):\n",
    "    \"\"\"\n",
    "    ScaleBase class for generating square root scale.\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'squareroot'\n",
    "\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        mscale.ScaleBase.__init__(self)\n",
    "\n",
    "    def set_default_locators_and_formatters(self, axis):\n",
    "        axis.set_major_locator(ticker.AutoLocator())\n",
    "        axis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        axis.set_minor_locator(ticker.NullLocator())\n",
    "        axis.set_minor_formatter(ticker.NullFormatter())\n",
    "\n",
    "    def limit_range_for_scale(self, vmin, vmax, minpos):\n",
    "        return  max(0., vmin), vmax\n",
    "\n",
    "    class SquareRootTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "\n",
    "        def transform_non_affine(self, a): \n",
    "            return np.array(a)**0.5\n",
    "\n",
    "        def inverted(self):\n",
    "            return SquareRootScale.InvertedSquareRootTransform()\n",
    "\n",
    "    class InvertedSquareRootTransform(mtransforms.Transform):\n",
    "        input_dims = 1\n",
    "        output_dims = 1\n",
    "        is_separable = True\n",
    "\n",
    "        def transform(self, a):\n",
    "            return np.array(a)**2\n",
    "\n",
    "        def inverted(self):\n",
    "            return SquareRootScale.SquareRootTransform()\n",
    "\n",
    "    def get_transform(self):\n",
    "        return self.SquareRootTransform()\n",
    "\n",
    "mscale.register_scale(SquareRootScale)\n",
    "'''\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.plot(np.arange(0, 9)**2, label='$y=x^2$')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_yscale('squareroot')\n",
    "ax.set_yticks(np.arange(0,9,2)**2)\n",
    "ax.set_yticks(np.arange(0,8.5,0.5)**2, minor=True)\n",
    "\n",
    "plt.show()\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/54/21/8b2ec99862903a6d3aed62ce156d21d114b8666e669c46d9e54041df9496/xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/envs/py3env/lib/python3.5/site-packages (from xgboost) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/envs/py3env/lib/python3.5/site-packages (from xgboost) (1.14.0)\n",
      "Installing collected packages: xgboost\n",
      "  Found existing installation: xgboost 0.6a2\n",
      "    Uninstalling xgboost-0.6a2:\n",
      "      Successfully uninstalled xgboost-0.6a2\n",
      "Successfully installed xgboost-0.81\n"
     ]
    }
   ],
   "source": [
    "!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/ft_satisfaction.csv -v df_ft_satis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_satis = pd.read_csv(StringIO(df_ft_satis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/ft_day_listen.csv -v df_ft_day_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft_day_listen = pd.read_csv(StringIO(df_ft_day_listen))\n",
    "df_train = pd.merge(df_ft_day_listen, df_ft_satis, how='left',on=['msno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/sub_ft_satisfaction.csv -v df_sub_ft_satis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/sub_ft_day_listen.csv -v df_sub_ft_day_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_ft_day_listen = pd.read_csv(StringIO(df_sub_ft_day_listen))\n",
    "df_sub_ft_satis = pd.read_csv(StringIO(df_sub_ft_satis))\n",
    "df_sub = pd.merge(df_sub_ft_day_listen, df_sub_ft_satis, how='left',on=['msno'])\n",
    "\n",
    "df_sub.is_churn = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/members_v3.csv -v members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_members =pd.read_csv(StringIO(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- missing value summary ----------\n",
      "msno                     0.000\n",
      "is_churn                 0.506\n",
      "six_month_day_listen     0.173\n",
      "six_month_satis          0.173\n",
      "city                     0.123\n",
      "bd                       0.123\n",
      "gender                   0.646\n",
      "registered_via           0.123\n",
      "registration_init_time   0.123\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_data = df_train.append(df_sub)\n",
    "df_data = pd.merge(df_data, df_members, how='left',on=['msno'])\n",
    "gender_encode = {'male':1, 'female':2}\n",
    "df_data.gender = df_data.gender.map(gender_encode)\n",
    "# change feature name\n",
    "df_data = df_data.rename(columns={'day_listen':'six_month_day_listen','user_latent_satisfaction':'six_month_satis'})\n",
    "# missing value \n",
    "print('-'*10,'missing value summary','-'*10)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(df_data.isnull().sum() / len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>six_month_day_listen</th>\n",
       "      <th>six_month_satis</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0nB9bTDIfGnfO+0TI5mI26X9oNuy/T4fIzND+D+RcQ4=</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2G9f1kCzNUsrpHaOKlHzK11vmtC9HfXmvYmHqAlr1Yc=</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.000</td>\n",
       "      <td>20150826.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QKXNX8FDdZNpmo50ENWER9/xdbjUGwO3euVsYoaYVu8=</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.000</td>\n",
       "      <td>20130825.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QMa4IOS+j5ryiIyE2ywjxjaxUeKFpFNNr8MCMzpbIsc=</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RM0u90xU9bIn+1P3yHVW9fuF2RvPrsEoaRJazS3sSy8=</td>\n",
       "      <td>0.000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  is_churn  \\\n",
       "0  0nB9bTDIfGnfO+0TI5mI26X9oNuy/T4fIzND+D+RcQ4=     0.000   \n",
       "1  2G9f1kCzNUsrpHaOKlHzK11vmtC9HfXmvYmHqAlr1Yc=     0.000   \n",
       "2  QKXNX8FDdZNpmo50ENWER9/xdbjUGwO3euVsYoaYVu8=     0.000   \n",
       "3  QMa4IOS+j5ryiIyE2ywjxjaxUeKFpFNNr8MCMzpbIsc=     0.000   \n",
       "4  RM0u90xU9bIn+1P3yHVW9fuF2RvPrsEoaRJazS3sSy8=     0.000   \n",
       "\n",
       "   six_month_day_listen  six_month_satis  city    bd  gender  registered_via  \\\n",
       "0                   nan              nan   nan   nan     nan             nan   \n",
       "1                   nan              nan 1.000 0.000     nan           7.000   \n",
       "2                   nan              nan 1.000 0.000     nan           7.000   \n",
       "3                   nan              nan   nan   nan     nan             nan   \n",
       "4                   nan              nan   nan   nan     nan             nan   \n",
       "\n",
       "   registration_init_time  \n",
       "0                     nan  \n",
       "1            20150826.000  \n",
       "2            20130825.000  \n",
       "3                     nan  \n",
       "4                     nan  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Member features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registered_via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f69cab63f28>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAF0CAYAAAAthjClAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XucXXV97//XMJNJQAKEMCgkXCIJHwly0UiwtWItikEpcFrEoFXwYC21qOfXo63+qtgHxXPw0dMKbdGKGAWqUsSfNkdBjPVSbUUCyMUEPpqEQC4CkQSIJJNMkvn9sVZws2fPZSd7ZzKzXs/HI4/stdZ3ffdnr+zs916X/V0d/f39SJKk6thntAuQJEl7luEvSVLFGP6SJFWM4S9JUsUY/pIkVYzhL0lSxXSNdgF7yrp1G/1NoySpMnp6JncMtsw9f0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlyRpCAsWXMv8+eeyYMG1o11Kyxj+kiQNord3M4sW3QbAokXford38yhX1BqGvyRJg+jr66O/vx+A/v4d9PX1jXJFrWH4S5JUMV3tfoKImAdcDXQC12XmlXXLTwOuAk4E5mfmLeX81wKfrGn6knL51yPiC8BrgKfLZRdl5r1tfSGSJI0TbQ3/iOgErgFeD6wGFkfEwsxcWtPsUeAi4AO162bm94CTy34OBpYB365p8sGdXxQkSdLItXvPfy6wLDNXAETETcA5wHPhn5kry2U7hujnPOC2zNzUvlIlSaqGdof/NGBVzfRq4NRd6Gc+8Pd18z4eEZcB/w58KDO37FqJkqS90SNLbx7tEnh20/OjZVX+Gy/Yb+IoVfMbR80+f7fWb3f4dzSY199MBxFxGHACcHvN7A8DjwHdwLXAXwKXD9XPlCn70dXV2cxTS5JG0dru0f/MnrDt+TVM6O5kwl5QV0/P5N1av93hvxo4omZ6OrC2yT7OB76Wmc/9viIzf1k+3BIRn6fueoFGNmzwjIEkjSV9W7ePdgkDaujbup2+rtGva926jcO2GeoLQrt/6rcYmBURMyKim+Lw/cIm+7gA+HLtjPJoABHRAZwL/KwFtUqSVAltDf/M3AZcSnHI/kHg5sxcEhGXR8TZABFxSkSsBt4MfCYiluxcPyKOpjhy8IO6rr8YEQ8ADwCHAFe083VIkjSetP13/pl5K3Br3bzLah4vpjgd0GjdlRQXDdbP/73WVilJUnU4wp8kSYPo6trnuSvXOzo66OoaH7E5Pl6FJEltMLF7Aq88ZRYAr3zFTCZ2Txjlilqj7Yf9JUkay8498xTOPfOU0S6jpdzzlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqpqvdTxAR84CrgU7gusy8sm75acBVwInA/My8pWbZduCBcvLRzDy7nD8DuAk4GLgHeHtmbm33a5EkaTxo655/RHQC1wBnArOBCyJidl2zR4GLgC816GJzZp5c/jm7Zv4ngE9m5ixgA3Bxy4uXJGmcavdh/7nAssxcUe6Z3wScU9sgM1dm5v3AjpF0GBEdwO8BO48QXA+c27qSJUka39p92H8asKpmejVwahPrT4qIu4BtwJWZ+XVgKvBUZm6r6XPacB1NmbIfXV2dTTy1JGk0re32M3swPT2Td2v9dod/R4N5/U2sf2Rmro2IFwPfjYgHgGd2pc8NGzY18bSSpNHWt3X7aJew11q3buOwbYb6gtDuw/6rgSNqpqcDa0e6cmauLf9eAXwfeBnwK+CgiNj5xaWpPiVJqrp2h/9iYFZEzIiIbmA+sHAkK0bElIiYWD4+BHgVsDQz+4HvAeeVTS8E/q3llUuSNE61NfzL8/KXArcDDwI3Z+aSiLg8Inb+bO+UiFgNvBn4TEQsKVc/DrgrIu6jCPsrM3NpuewvgT+PiGUU1wB8rp2vQ5Kk8aSjv7+ZU/Bj17p1G6vxQiVpnHhk6c2jXcJe66jZ5w/bpqdncqPr7gBH+JMkqXIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwl6QxbMGCa5k//1wWLLh2tEvRGGL4S9IY1du7mUWLbgNg0aJv0du7eZQr0lhh+EvSGNXX10d/fz8A/f076OvrG+WKNFYY/pIkVYzhL0lSxXSNdgGSNNZ845Y7R7sEALZs2fS86W8vvIeJE/cbpWp+46zz5o52CRqGe/6SJFWM4S9JUsUY/pIkVYzhL0lj1D6dv7lsq4OO501LQzH8JWmMmtDVzbHHzAFg1jEvZ0JX9yhXpLHCr4mSNIbNfdkbmPuyN4x2GRpj3POXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9Jkiqm7YP8RMQ84GqgE7guM6+sW34acBVwIjA/M28p558MfBo4ANgOfDwz/7Vc9gXgNcDTZTcXZea97X4tkiSNB23d84+ITuAa4ExgNnBBRMyua/YocBHwpbr5m4B3ZObxwDzgqog4qGb5BzPz5PKPwS9J0gi1e89/LrAsM1cARMRNwDnA0p0NMnNluWxH7YqZ+fOax2sj4gmgB3iqzTVLkjSutfuc/zRgVc306nJeUyJiLtANLK+Z/fGIuD8iPhkRE3evTEmSqqPde/4dDeb1N9NBRBwG3AhcmJk7jw58GHiM4gvBtcBfApcP1c+UKfvR1dXZzFNLUkMTJvhZMpSenskt6Wdtt9t5MLu7jdsd/quBI2qmpwNrR7pyRBwAfBP4SGbesXN+Zv6yfLglIj4PfGC4vjZs2DTSp5WkIfX1bR/tEvZq69ZtbEk/fVvdzoMZyTYe6gtCuw/7LwZmRcSMiOgG5gMLR7Ji2f5rwA2Z+ZW6ZYeVf3cA5wI/a2nVkiSNY20N/8zcBlwK3A48CNycmUsi4vKIOBsgIk6JiNXAm4HPRMSScvXzgdOAiyLi3vLPyeWyL0bEA8ADwCHAFe18HZIkjScd/f1NnYIfs9at21iNFyqp7b5xy52jXcJe7azz5rakn0eW3tySfsajo2afP2ybnp7Jja67AxzhT5KkyjH8JUmqGMNfkqSKMfwlSaoYw1+SpIox/CVJqhjDX5KkijH8JUmqmKbDPyK804IkSWPYiG7sExGvAf4YOB04NCL6gPuBrwKfzcz17StRkiS10rB7/hHxTeD9FDfkmQtMAnooxuzvBr4TEfPaWaQkSWqdkez5/0VmLqmb1wfcCdwZEVcCR7W8MkmS1BbDhn+D4K9f3gcsa1lFkiSprUZ0zh8gIgL4CHBM7XqZ2ZrbN0mSpD1ixOEP3AR8Bfg8sL095UiSpHZrJvz3ycz/1bZKJEnSHtHM7/x/HBEntq0SSZK0RzSz538q8M6ISKB350zP+UuSNLY0E/7/o21VSJKkPWakI/x1An+UmX/c5nokSVKbjeicf2Zup/iJnyRJGuOaOez/3Yj4J+AG4Nc7Z2bm0pZXJUmS2qaZ8L+4/PtNNfP6gRe3rhxJktRuIw7/zJzRzkIkSdKe0czwvrMbzfewvyRJY0szh/2/WfN4EvBC4BHAIwKSJI0hu3zYPyJOB+a1vCJJktRWzQzv+zyZ+e/Aq1pYiyRJ2gN29Zz/PsApwIEtr0iSJLXVrp7z3wYsAy5sbTmSJKnd/KmfJEkV08yePxFxDMUwv8+tl5m3trooSZLUPs2c8//fwLuAB4Ht5ex+wPCXJGkMaWbP/83AMZn5TLuKkSRJ7dfMT/1+afBLkjT2DbvnHxFvLB/+OCK+DHwF6N25fLhz/hExD7ga6ASuy8wr65afBlwFnAjMz8xbapZdCHyknLwiM68v588BvgDsS3Ha4f2Z2T/ca5EkSSM77P/Buun31jwe8px/RHQC1wCvB1YDiyNiYd39AB4FLgI+ULfuwcDHgFeUz3N3ue4G4NPAu4E7yuefB9w2gtciSVLlDRv+mfna3eh/LrAsM1cARMRNwDnAc+GfmSvLZTvq1n0DsCgz15fLFwHzIuL7wAGZ+eNy/g3AuRj+kiSNSDNX+78d+Ea5571zz/zMzPziEKtNA1bVTK8GTh3hUzZad1r5Z3WD+UOaMmU/uro6R/jUkjS4CRP8LBlKT8/klvSzttvtPJjd3cbNXO3/gcy8cedEZq6PiA8AQ4V/R4N5Iz03P9i6u9Tnhg2bRvi0kjS0vr7twzeqsHXrNrakn76tbufBjGQbD/UFYZdv7FMa7mvZauCImunpwNoR9j3YuqvLx7vSpyRJlddM+D8WEX+wcyIi/hB4Yph1FgOzImJGRHQD84GFI3y+24EzImJKREwBzgBuz8xfAhsj4pUR0QG8A/i3Jl6HJEmV1kz4vx/43xGxLCKWAR8H/myoFTJzG3ApRZA/CNycmUsi4vKIOBsgIk6JiNUUgwh9JiKWlOuuB/6G4gvEYuDynRf/AX8KXEdxc6HleLGfJEkj1tHfP/Kfx5c/3QuK8+4PZeb2mmUnZub9rS+xNdat2+g4AJJa4hu33DnaJezVzjpvbkv6eWTpzS3pZzw6avb5w7bp6Znc6Bo5oMkb+5Rhv3SQxV8AXt5Mf5Ikac/b3Qv+ag36DUOSJO09Whn+HlaXJGkMaGX4S5KkMcDD/pIkVUwrw/+fWtiXJElqk2bG9v874HLgWeB7FFf2/0lm/gtAZn6uLRVKkqSWambP/3WZ+TTF3fbWALOouw2vJEna++3KYf/TgP8vM9fiFf6SJI05zYT/ExHxWYrx+RdFRBdNDhIkSZJGXzPh/1ZgCfCWzNxAcTe9v2tLVZIkqW1GvOeemeuAq2qmV1IM6StJksaQYcM/Im7MzLdHxGIanOPPzNbcwUGSJO0RI9nz37m375X9kiSNA8OGf2beXf79g/aXI2k8WbDgWr797Vs544w38t//+7tHuxxJpWEv+IuIf4iIw4ZYfk5EzG9tWZLGut7ezSxadBsAixZ9i97ezaNckaSdRnLY/zvA7RGxDvgJ8DgwCQiK3/wvAj7StgoljUl9fX309xeXCfX376Cvr49Jk/Yd5aokwcgO+y8EFkbE7wC/CxwHbAZ+BHwoM59oa4WSmvLQp/5htEsAYFNf3/Omf7Hgs+w3YcIoVfMbL3nP+0a7BGnUNfNTvx9RBL4kSRrDmhqhLyJOB46pXS8zP9XqoiSNfZ37/OYu3x1105JGVzN39bsemAPcA2wvZzu2v6SGJnZ2ccrh01i8dg2vOHwaEzsdDVzaWzTzv/G3gOMzs2/YlpIEvGlm8KaZMdplSKrTzNj+q9pWhSRJ2mOa2fP/OfDvEfF1oHfnTM/5S5I0tjQT/pOA5cAJNfM85y9J0hjTzE/93tnOQiRJ0p7R7E/9AjiJ4igAAJl5Q6uLkiRJ7dPMT/3eB/wJcBiwGHg18APA8JckaQxp5mr/dwNzgUcz8w3l4w1tqUqSJLVNM+Hfm5nPAvtEREdm/oxitD9JkjSGNHPOf1NETADuAz4REauA/dpTliRJapdm9vzfA3QD/xM4GHgN8PZ2FCVJktqnmZ/6/ax8+CzwrvaUI0mS2m3Ee/4RMSsifhQRD5fTL4+Iv25bZZIkqS2aOef/aeAK4Mpy+l7gRuCvh1opIuYBVwOdwHWZeWXd8okUPxecAzwJvCUzV0bE24AP1jQ9EXh5Zt4bEd+n+Mnh5nLZGZn5RBOvRZKkymrmnP+BmfktyiF9M3MHsHWoFSKiE7gGOBOYDVwQEbPrml0MbMjMmcAngU+U/X8xM0/OzJMpri1YmZn31qz3tp3LDX5JkkaumfDfXl7t3w8QEdOAHcOsMxdYlpkrMnMrcBNwTl2bc4Dry8e3AKdHREddmwuALzdRqyRJGkQz4f8p4GvAIeW5/h8C/2eYdabx/FsBry7nNWyTmduAp4GpdW3ewsDw/3xE3BsRH23wZUGSJA2imav9b4iIFcDvU/y+/8LM/OEwqzUK5fo7AQ7ZJiJOBTbV/NoAikP+ayJiMvBVitMCQw4zPGXKfnR1dQ5TrjT2Le9u6pYdldPTM3m3+5gwwc+SobRiGwOs7XY7D2Z3t3FTnxKZ+SPgR02ssho4omZ6OrB2kDarI6ILOBBYX7N8PnV7/Zm5pvx7Y0R8ieL0wpDhv2HDpibKlsauvq3bRruEvdq6dRt3u4++vu0tqGT8asU2Bujb6nYezEi28VBfEJq5sU8AfwXMrF0vM+cOsdpiYFZEzADWUAT5W+vaLAQuBH4MnAd8NzN3XlewD/Bm4LSaOrqAgzLzV+U1CGcB3xnp65Akqeqa2fP/CsVP+74AjOjrWGZui4hLgdspfuq3IDOXRMTlwF2ZuRD4HHBjRCyj2OOfX9PFacDqzFxRM28icHsZ/J0Uwf/ZJl6HJEmV1kz4b8vMv232CTLzVuDWunmX1Tzupdi7b7Tu94FX1s17lmJMAEmStAuaudr/W+WAPZIkaQxrZs//O8C/RcQOYAvFVfr9mXloWyqTJElt0Uz4Xwu8E7iHEZ7zlyRJe59mwn99Zt7StkokSdIe0Uz4fz0iLgFuBnp3zsxMf0AvSdIY0kz4X1H+/SmKEfg6yr8dgkmSpDGkmeF9m/llgCRJ2ksZ6JIkVYzhL0lSxRj+kiRVjOEvSVLFGP6SJFWM4S9JUsUY/pIkVYzhL0lSxRj+kiRVjOEvSVLFGP6SJFWM4S9JUsUY/pIkVYzhL0lSxRj+kiRVjOEvSVLFGP6SJFWM4a/KWrDgWubPP5cFC64d7VIkaY8y/FVJvb2bWbToNgAWLfoWvb2bR7kiSdpzDH9VUl9fH/39/QD09++gr69vlCuSpD3H8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKqar3U8QEfOAq4FO4LrMvLJu+UTgBmAO8CTwlsxcGRFHAw8CWTa9IzMvKdeZA3wB2Be4FXh/Zva3+7VIkjQetHXPPyI6gWuAM4HZwAURMbuu2cXAhsycCXwS+ETNsuWZeXL555Ka+Z8G3g3MKv/Ma9drkCRpvGn3Yf+5wLLMXJGZW4GbgHPq2pwDXF8+vgU4PSI6BuswIg4DDsjMH5d7+zcA57a+dEmSxqd2H/afBqyqmV4NnDpYm8zcFhFPA1PLZTMi4qfAM8BHMvOHZfvVdX1OG66QKVP2o6urc5dehMaf7u4dz5ueOnV/Djxw8ihV01rLu9t+Nm9M6+nZ/X/nCRP8LBlKK7YxwNput/Ngdncbt/tTotEefP25+cHa/BI4MjOfLM/xfz0ijh9hnwN8/evfbDh/xowXc+SRRwHwwAP3sX79+gFtDjroIE466WUArF69iuXLlzXs69Wvfg377LMPv/71r7n77sUN2xx//AkccsghANxxx3+xZcuWAW2mTZvOzJmzAMh8iMce++WANvvuux9z5xbfox5//HEeemhpw+c79dTfYtKkSfT19fFf//Wjhm2OPTY47LDDAbjnnrvYuHHjgDaHHNLD8ce/FIAVK5azatWjA9p0dXXxqle9GoCnntrAfffdO6DNN5bexrRZRzLpBfsWfd3/C/p37BjQbsqLpnLwi4rt9NjDa3j26V8PaDPpBfsybdaRADy9bgO/WvNEw9d39Akz6ezsZOuWrax68GEAtm3d9rx/68v/9e846EUHA/DIgyvYtmXgjX4OmHoQPUe8EIB1qx7nmSefGrgNJk7gqONeDMCvn9rI4yvXNqzpiONm0D2xm+3bt7PygeL9dNbsM5/XZubMWUybNh2An/70bp555pkB/UydeggvfekJAKxc+TCPPLKSVQ8+9Lw2HR0dzHnxMUVNvb08tGb1gH4A4vBpTN63+Hf56cMr2N7g3+VFB01h+tTiu/nyxx5jw7MD/132mziJ2dOLutc98wyPrGv873Ly0TPo6uxkS18fDzz6SMM2Mw59IVMnFx9yDzz6KFv6tg5oc8jkAzj60EMBePRX63ji6acHtJnQ1cVJRx0NwIMPPsySJQ80fL45c05h//33Z8eOHfzwhz9o2OaYY2bS17cdgJWP/pxnNw38/7L/Cw7gqCOK/8O/evIxHl+3pmFfx79kDgCbNz/LikceatjmqOkz2X//AwHIn9/Hth3bBrQ55OAX8sJDi22+eu3DPP3MwM+xSRP35ZgZxVnXp55+kjW/XNnw+Y495gQmTOimr28rP1/eeDtNO+xoDjqwfB88vJTeLc+/K+akW57h0ENfyHHHFc+3YsUyVq1aNaCfCRMm8Nu//TsArF//JA88cP/zlj/+SPH8xx83g/3Lz4yf3NX4s+6IaYdy+GHFZ0b+4lGeavCZccDk/Tgujgbgscef5JFVjzfs65SXv4R99tmHTZt6eWDpioZtjp15BFMOKt6bP73/52zdOvDf5YWHHszRR74IgBUr17LuVwM/MyZN6uakl84E4FdPPs3yhxu/V04+YRYTJ05g27Zt3H3vz/nFmoUDa6r7LH/DG17bsC9o/2H/1cARNdPTgfpPw+faREQXcCCwPjO3ZOaTAJl5N7AcOLZsP32YPiVJ0iA6dt7WtB3KMP85cDqwBlgMvDUzl9S0+TPghMy8JCLmA3+QmedHRA/Fl4DtEfFi4Idlu/URsRh4L/ATiqv9/zEzbx2qlnXrNvprgL3AP//4c6NdAgB9m7fyn9fe/tz0q979Bibs2z2KFcElv3VxS/p56FP/0JJ+xquXvOd9u93HN265swWVjF9nnTe3Jf08svTmlvQzHh01+/xh2/T0TB70+rm27vln5jbgUuB2ip/t3ZyZSyLi8og4u2z2OWBqRCwD/hz4UDn/NOD+iLiP4kLASzJz57GsPwWuA5ZRHBG4rZ2vQ5Kk8aTtVwaVe+S31s27rOZxL/DmBut9FfjqIH3eBby0tZVKklQNjvAnSVLFGP6SJFWM4S9JUsUY/pIkVYzhL0lSxRj+kiRVjOEvSVLFGP6SJFWM4S9JUsUY/qqkjs6at35H3bQkjXN+4qmSurq7OPzEowE4/ISj6epu+0jXkrTX8BNPlXXsa0/g2NeeMNplSNIe556/JEkVY/hLklQxhr8kSRVj+EuSVDGGvyRJFWP4S5JUMYa/JEkVY/hLklQxhr8kSRVj+EuSVDGGvyRJFWP4S5JUMYa/JEkVY/hLklQxhr8kSRVj+EuSVDGGvyRJFWP4S5JUMYa/JEkVY/hLklQxhr8kSRVj+EuSVDFd7X6CiJgHXA10Atdl5pV1yycCNwBzgCeBt2Tmyoh4PXAl0A1sBT6Ymd8t1/k+cBiwuezmjMx8ot2vRZKk8aCte/4R0QlcA5wJzAYuiIjZdc0uBjZk5kzgk8Anyvm/An4/M08ALgRurFvvbZl5cvnH4JckaYTafdh/LrAsM1dk5lbgJuCcujbnANeXj28BTo+Ijsz8aWauLecvASaVRwkkSdJuaPdh/2nAqprp1cCpg7XJzG0R8TQwlWLPf6c/BH6amVtq5n0+IrYDXwWuyMz+oQqZMmU/uro6d+1VqGUmTPDfYDA9PZNb0s/y7rafzRvTWrGdfR8PrVXv5bXdbufB7O42bvenREeDefUhPWSbiDie4lTAGTXL35aZayJiMkX4v53iuoFBbdiwaUQFq736+raPdgl7rXXrNrakn76t21rSz3jViu3s+3horXsvu50HM5JtPNQXhHYf9l8NHFEzPR1YO1ibiOgCDgTWl9PTga8B78jM5TtXyMw15d8bgS9RnF6QJEkj0O7wXwzMiogZEdENzAcW1rVZSHFBH8B5wHczsz8iDgK+CXw4M/9zZ+OI6IqIQ8rHE4CzgJ+1+XVIkjRutPWwf3kO/1Lgdoqf+i3IzCURcTlwV2YuBD4H3BgRyyj2+OeXq18KzAQ+GhEfLeedATwL3F4GfyfwHeCz7XwdkiSNJ22/MigzbwVurZt3Wc3jXuDNDda7ArhikG7ntLJGSZKqxBH+JEmqGMNfkqSKMfwlSaoYw1+SpIox/CVJqhjDX5KkijH8JUmqGMNfkqSKMfwlSaoYw1+SpIox/CVJqhjDX5KkijH8JUmqGMNfkqSKMfwlSaoYw1+SpIox/PdCCxZcy/z557JgwbWjXYokaRwy/Pcyvb2bWbToNgAWLfoWvb2bR7kiSdJ40zXaBexN/vGm/xztEti+bSv9/f0A9Pf388+33EFnV/coVwXvnf+q0S5BktQi7vnvZTq7unnRjJcB8KIZJ+8VwS9JGl/c898LHXPS6zjmpNeNdhmSpHHKPX9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKsbwlySpYgx/SZIqxvCXJKliDH9Jkiqm7Xf1i4h5wNVAJ3BdZl5Zt3wicAMwB3gSeEtmriyXfRi4GNgOvC8zbx9Jn5IkaXBt3fOPiE7gGuBMYDZwQUTMrmt2MbAhM2cCnwQ+Ua47G5gPHA/MAz4VEZ0j7FOSJA2i3Yf95wLLMnNFZm4FbgLOqWtzDnB9+fgW4PSI6Cjn35SZWzLzYWBZ2d9I+pQkSYNod/hPA1bVTK8u5zVsk5nbgKeBqUOsO5I+JUnSINp9zr+jwbz+EbYZbH6jLyz1fQ7Q0zO5UX/Pc/l75w3XRLvpo2f/j9EuYdzr+dhfjXYJ4947//T00S6hEnpec/FolzButXvPfzVwRM30dGDtYG0iogs4EFg/xLoj6VOSJA2i3Xv+i4FZETEDWENxAd9b69osBC4EfgycB3w3M/sjYiHwpYj4e+BwYBZwJ8URgeH6lCRJg2jrnn95Dv9S4HbgQeDmzFwSEZdHxNlls88BUyNiGfDnwIfKdZcANwNLgW8Bf5aZ2wfrs52vQ5Kk8aSjv3/Y0+WSJGkccYQ/SZIqxvCXJKli2j68rwYXEQH8a82sFwOXZeZrLGiVAAAHDklEQVRVNW06KIYyfiOwCbgoM+/Zo4WOQRGxADgLeCIzX1rO+xuKAaF2AE9QbMsBvxSJiAuBj5STV2Tm9fVtNFBEvB/4Y4qLcj9b+z4ul/te3g0RMQn4D2AixWf3LZn5sbo2gw6XrpErR5K9C1iTmWfVLRsX29g9/1GUhZMz82SKN9Im4Gt1zc6k+KXDLODdwKf3bJVj1hcohoWu9beZeWK5vb8BXFa/UkQcDHwMOJViNMmPRcSUNtc65kXESymCfy5wEnBWRMyqa+Z7efdsAX4vM08CTgbmRcQr69o0HC5dTXs/xQXljYyLbWz47z1OB5Zn5iN1888BbsjM/sy8AzgoIg7b8+WNLZn5HxTjRdTOe6Zm8gU0HhzqDcCizFyfmRuARQz8EqGBjgPuyMxN5S9yfgD8t7o2vpd3Q7ndfl1OTij/1L+HBxsuXSMUEdOBNwHXDdJkXGxjw3/vMR/4coP5DmfcQhHx8YhYBbyNBnv+uL131c+A0yJiakTsR3Fo/4i6Nm7b3VTe3OxeitNWizLzJ3VNBhsuXSN3FfAXFKcHGxkX29jw3wtERDdwNvCVBotHMkSyRigz/yozjwC+SDFeRD239y7IzAcpDn8uohiX4z5gW10zt+1uKsc6OZliZNO55emWWm7j3RARO68TunuIZuNiGxv+e4czgXsy8/EGyxzOuD2+BPxhg/lu712UmZ/LzJdn5mkUp1x+UdfEbdsimfkU8H0GnpIabLh0jcyrgLMjYiXFHWN/LyL+pa7NuNjGhv/e4QIaH/KHYvjjd0RER3lxz9OZ+cs9V9r4UXcB2tnAQw2a3Q6cERFTygv9zijnaRgRcWj595HAHzDwPe17eTdERE9EHFQ+3hd4HQPfwzuHS4ea4dL3XJVjW2Z+ODOnZ+bRFKdiv5uZf1TXbFxsY3/qN8rK86OvB/6kZt4lAJn5z8CtFOdPl1H8GuCdo1DmmBMRXwZ+FzgkIlZTXMH/xvLnlTuAR4BLyravAC7JzHdl5vryJ4GLy64uz8wx961+lHw1IqYCfRTDcW/wvdxShwHXlz9D24diaPNvRMTlwF2ZuZBiuPQby+HS11MEmHbTeNzGDu8rSVLFeNhfkqSKMfwlSaoYw1+SpIox/CVJqhjDX5KkijH8JUmqGMNf0nMi4pKI+H+GaXNuRMzdQ/WsbDCE7UjXvTUijml1TdJ44CA/0jgTEV3lDUeaVg7GM5xzKe51fmez/UdEZ2Zub7qwXZCZb9wTzyONRYa/NA5ERD/FncjeBPwQ+GhE/AXF8KNdwBrgjzPzsYg4EFgAHF/OX0NxM5MPRMRfA/uXj38b+CeKI4QTgCsoRjQ7G3hdRLwL+PvMvCEiLgTeUz7X08CfZmZGxEUUI6CtA2YDF0fE48A/AkcC+wJfzsz/Vb6OVwOfAjYDd9D4Jiq1r/sXwHmZeV85/V7g5Zn5znJ89rMy82cR8T/LOrqA3rK+e3dhU0vjgof9pfFjn8z83cz8aET8ETATeGVmvpxiaN2/K9tdBmzIzJcAbwZePUh/fwl8sryL3EuB2zLzdoqxza/MzJPL4H81cD5wWmbOAf6W4svFTr8D/HVmzikD9wbgHzJzLjAHODMiXh8REylupvLectmPKL4gDOUGfjPOOsBFwOcbtcvMUzLzZcBHgZEc4ZDGLff8pfHj+prHZwOvAO4pbmfw3B45wGuB9wKU9zL4+iD9fQ/4cEQcReN7x+/0+8BJwE/K5+oAptQs/1FmLgeIiBdQ3HOhp2wLMBk4Dngc2JSZ3y9ruzkirh3Ba/5JeZTjJRR3WPthg3ZzIuL/BQ6muLfDscP0K41rhr80fvy65nEHcEVmLmjQroMR3H88M6+KiP9Lcfe4f4yIb2fmRwbpb0FmXjaCuvYpn/uUzOyrbRQRJw1XU4MaH42IpRS3xf5d4Pr6O6xFRDdwC8WRiXsi4nCKUx1SZXnYXxqfFgLvKW9LTERMrAnX71EeKi+Xn9Oog4g4NjOXZ+ZngKuBnVf4P0Oxh73T/6W4Ve/0cr3OiJjTqM/M3EixZ/6hmuc5IiJeRHF72n0j4rRy/nl1zzOYLwDvorg19vUNlk+i2NFZVU6/ZwR9SuOa4S+NQ5l5I/BF4AcRcT9wN/CqcvHlwKERsQT4F+A/+c0pgVrvi4glEfFTitMEf1XOvxF4a0TcGxHvyMz/KJctjIj7gJ8xyBeK0tuA2RHxQEQ8APwrcFBmbqEI8Gsi4k6K0xaPjuDlfpVir39pZg5on5nPUFznsDgi/gN4dgR9SuOat/SVKiYiJgCdmdkbEQdQXFj355n5nVEuTdIe4jl/qXqmALdFRCfFIfEvGfxStbjnL2mvFxF3MXBn5Y7MvGQ06pHGOsNfkqSK8YI/SZIqxvCXJKliDH9JkirG8JckqWIMf0mSKub/B+j7YRS6uatKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f69e6bd6080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# aggregate result in order to sort \n",
    "result_reg = df_data.groupby([\"registered_via\"])['is_churn'].mean().reset_index().sort_values('is_churn')\n",
    "# make plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.barplot(x='registered_via',y='is_churn',data=df_data,order=result_reg['registered_via'],ax=ax,alpha=.75)\n",
    "ax.axhline(0.049,linestyle='dashed', c='black',alpha = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.registered_via = df_data.registered_via.fillna(-1)\n",
    "df_data.six_month_day_listen = df_data.six_month_day_listen.fillna(-1)\n",
    "df_data.six_month_satis = df_data.six_month_satis.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_rf(training_data,testing_data):\n",
    "  # splits train and validation set\n",
    "  X = training_data.drop(labels=['msno','is_churn'],axis=1)\n",
    "  Y = training_data['is_churn']\n",
    "  X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,random_state = 2)\n",
    "  # Training ~ 01:45s\n",
    "  model = RandomForestClassifier(random_state=2,n_estimators=300,\n",
    "                           min_samples_split=0.05,n_jobs=-1,class_weight={0 :0.45,1 :0.55})\n",
    "  model.fit(X_train,Y_train)\n",
    "  \n",
    "  # caculating E_val\n",
    "\n",
    "  model_probs = model.predict_proba(X_val)\n",
    "    # [:,1] to show the prob to is_churn = 1\n",
    "  model_val_score = log_loss(Y_val,model_probs[:,1])\n",
    "  \n",
    "  # predict on testing set\n",
    "  model_pred_testing_set = model.predict_proba(testing_data.drop(labels=['msno','is_churn'],axis=1))\n",
    "  model_pred_testing_set = model_pred_testing_set[:,1] # take out the prob if is_churn = 1\n",
    "  submission = pd.DataFrame({\"msno\": testing_data.msno})\n",
    "  submission.insert(1,column='is_churn',value=model_pred_testing_set)\n",
    "  \n",
    "  return model, model_val_score, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_xgb(training_data,testing_data):\n",
    "  # splits train and validation set\n",
    "  X = training_data.drop(labels=['msno','is_churn'],axis=1)\n",
    "  Y = training_data['is_churn']\n",
    "  X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,random_state = 2)\n",
    "  # model\n",
    "  xgb_watchlist = [(X_train, Y_train), (X_val, Y_val)]\n",
    "  model = xgb.XGBClassifier(learning_rate=0.08, max_depth=4,n_estimators=300,\\\n",
    "                 subsample=0.5, seed=2,missing=-1)\n",
    "  model.fit(X_train, Y_train,eval_set=xgb_watchlist,eval_metric='logloss',\n",
    "            early_stopping_rounds=20,verbose=70)\n",
    "  # caculating E_val\n",
    "\n",
    "  model_probs = model.predict_proba(X_val)\n",
    "    # [:,1] to show the prob to is_churn = 1\n",
    "  model_val_score = log_loss(Y_val,model_probs[:,1])\n",
    "  \n",
    "  # predict on testing set\n",
    "  model_pred_testing_set = model.predict_proba(testing_data.drop(labels=['msno','is_churn'],axis=1))\n",
    "  model_pred_testing_set = model_pred_testing_set[:,1] # take out the prob if is_churn = 1\n",
    "  submission = pd.DataFrame({\"msno\": testing_data.msno})\n",
    "  submission.insert(1,column='is_churn',value=model_pred_testing_set)\n",
    "  \n",
    "  return model, model_val_score, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['msno', 'is_churn', 'six_month_day_listen', 'six_month_satis', 'city',\n",
      "       'bd', 'gender', 'registered_via', 'registration_init_time'],\n",
      "      dtype='object')\n",
      "Index(['msno', 'is_churn', 'six_month_day_listen', 'six_month_satis', 'city',\n",
      "       'bd', 'gender', 'registered_via', 'registration_init_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_train, df_sub = df_data[:len(df_train)], df_data[len(df_train):]\n",
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "reg_via_fts = ['msno','is_churn','six_month_day_listen', 'six_month_satis','registered_via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_via_model, reg_via_val_score,\\\n",
    "reg_via_pred = model_training_rf(df_train[reg_via_fts],df_sub[reg_via_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_via_model_xgb, reg_via_val_score_xgb,\\\n",
    "reg_via_pred_xgb = model_training_xgb(df_train[reg_via_fts],df_sub[reg_via_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('day_listen + user_letent_satisfaction logloss val: ',0.192158)\n",
    "print('reg_via log_loss val xgb: ',np.around(reg_via_val_score_xgb,decimals=6))\n",
    "print('reg_via log_loss val rf: ',np.around(reg_via_val_score,decimals=6))\n",
    "print('-'*20,'LB','-'*20)\n",
    "print('reg_via log_loss LB : ',0.13264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_via_pred_xgb.to_csv(\"/content/datalab/sub_reg_via_xgb_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp '/content/datalab/sub_reg_via_xgb_pred.csv' 'gs://kk_data/submission/sub_reg_via_xgb_pred.csv';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_data[['bd']].describe())\n",
    "# how many % of meanful values? \n",
    "meaningful = (df_data.bd > 0 ) & (df_data.bd < 90)\n",
    "print('Ages between 0 ~ 90   :  %.3f   ' %( len(df_data[meaningful]) / len(df_data) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.56)\n",
    "from pandas.plotting import table\n",
    "\n",
    "# create fig and ax \n",
    "with sns.axes_style('white'):\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(1,2, figsize=(25,10))\n",
    "# distribution\n",
    "    NOT_churn = ( (df_data.is_churn == 0) & (df_data.bd > 0 ) & (df_data.bd < 90) )\n",
    "    churn = ( (df_data.is_churn == 1) & (df_data.bd > 0 ) & (df_data.bd < 90) )\n",
    "    bins= 25\n",
    "    sns.distplot(df_data.loc[NOT_churn,'bd'],kde=False,label='NOT_churn',ax=ax1,norm_hist=True,bins=bins)\n",
    "    sns.distplot(df_data.loc[churn,'bd'],kde=False,label='churn',ax=ax1,norm_hist=True,bins=bins)\n",
    "    ax1.legend()\n",
    "    ax1.axvline(26,linestyle='dashed',c='black',alpha=.5)\n",
    "\n",
    "# feature engineering\n",
    "# create feature  bd<26 == 1, bd>26 ==0 , other = -1\n",
    "    condition = df_data.bd < 26\n",
    "    df_data['age_under_26'] = df_data.bd.apply(lambda x: 1 if 0 < x <= 26 else 0 if 26 < x <= 90 else -1)\n",
    "\n",
    "# prepare table (% of each value)\n",
    "    df_percent = pd.DataFrame( df_data['age_under_26'].value_counts().sort_index() / len(df_data) ).round(3) \n",
    "# barplot with percent table\n",
    "    sns.barplot(x='age_under_26',y='is_churn',data=df_data,ax=ax2)\n",
    "    ax2.axhline(0.049,linestyle='dashed', c='black',alpha = .3)\n",
    "    table(ax2, df_percent,bbox=[0.035, 0.72, 0.2, 0.27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and prepare features \n",
    "df_train = df_data[:len(df_train)]\n",
    "df_sub = df_data[len(df_train):]\n",
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "age_fts = reg_via_fts + ['age_under_26']\n",
    "exclude_list = ['msno','is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training using features:' ,[fts for fts in age_fts if fts not in exclude_list])\n",
    "age_model_xgb, age_val_score_xgb,\\\n",
    "age_pred_xgb = model_training_xgb(df_train[age_fts],df_sub[age_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_pred_xgb.to_csv(\"/content/datalab/sub_age_xgb_pred.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/sub_age_xgb_pred.csv' 'gs://kk_data/submission/sub_age_xgb_pred.csv';\n",
    "\n",
    "print('reg_via logloss val: ',0.164043)\n",
    "print('reg_via logloss LB: ',0.13264)\n",
    "print('-'*20,' + age','-'*20)\n",
    "print('age log_loss LB : ',0.161515)\n",
    "print('age log_loss LB : ',0.13161)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions features\n",
    "## IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# first time you for merging data\n",
    "%gcs read --object gs://kk_data/transactions.csv -v trans\n",
    "\n",
    "%gcs read --object gs://kk_data/transactions_v2.csv -v trans_v2\n",
    "\n",
    "%%time\n",
    "df_trans = pd.read_csv(StringIO(trans))\n",
    "df_trans_v2 = pd.read_csv(StringIO(trans_v2))\n",
    "df_trans_all = pd.concat([df_trans,df_trans_v2])\n",
    "df_train_trans = pd.merge(df_train[['msno','is_churn']],df_trans_all,how='left',on=['msno'])\n",
    "df_sub_trans = pd.merge(df_sub[['msno','is_churn']],df_trans_all,how='left',on=['msno'])\n",
    "\n",
    "df_train_trans.to_csv(\"/content/datalab/df_train_trans_merged.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/df_train_trans_merged.csv' 'gs://kk_data/df_train_trans_merged.csv';\n",
    "\n",
    "df_sub_trans.to_csv(\"/content/datalab/df_sub_trans_merged.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/df_sub_trans_merged.csv' 'gs://kk_data/df_sub_trans_merged.csv';\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/df_train_trans_merged.csv -v trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/df_sub_trans_merged.csv -v sub_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train_trans = pd.read_csv(StringIO(trans))\n",
    "df_sub_trans = pd.read_csv(StringIO(sub_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trans.shape, df_sub_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_compressor(df):\n",
    "  #object \n",
    "  # when unique ratio > 0.5, catogory stands less memory than object\n",
    "  converted_obj = pd.DataFrame()\n",
    "  df_obj = df.select_dtypes(include=['object'])\n",
    "  for col in df_obj.columns:\n",
    "    converted_obj.loc[:,col] = df_obj.loc[:,col].astype('category')\n",
    "\n",
    "  # numbers\n",
    "  df_down_num = pd.DataFrame()\n",
    "  df_num = df.select_dtypes(include=['number'])\n",
    "  for col in df_num.columns:\n",
    "    df_down_num.loc[:,col] = pd.to_numeric(df_num.loc[:,col],downcast='signed')\n",
    "\n",
    "  # merge\n",
    "  df = pd.concat([converted_obj,df_down_num],axis=1)\n",
    "  \n",
    "  del converted_obj, df_obj, df_down_num, df_num  \n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_datetime(s):\n",
    "    dates = {date:pd.to_datetime(date,format='%Y%m%d') for date in s.unique()}\n",
    "    return s.map(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "date_rename = {'transaction_date':'trans_date', 'membership_expire_date':'mem_expire_date'}\n",
    "df_train_trans.rename(columns=date_rename,inplace=True)\n",
    "df_sub_trans.rename(columns=date_rename,inplace=True)\n",
    "\n",
    "# converting\n",
    "df_train_trans = dtype_compressor(df_train_trans)\n",
    "df_sub_trans = dtype_compressor(df_sub_trans)\n",
    "# 1 mins 40 s (with apply approach)\n",
    "# vectorlized 15s\n",
    "d = ['trans_date', 'mem_expire_date']\n",
    "for col in d:\n",
    "  df_train_trans[col] = fast_datetime(df_train_trans[col])\n",
    "  df_sub_trans[col] = fast_datetime(df_sub_trans[col])\n",
    "# took 6s \n",
    "# total about 20~30s for single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_trans.is_churn = np.NaN\n",
    "print(\"Traning_set missing:\\n\",np.around(df_train_trans.isnull().sum() / df_train_trans.shape[0],decimals=3))\n",
    "print(''*60)\n",
    "print(\"_\"*60)\n",
    "print(''*60)\n",
    "print(\"Testing_set missing:\\n\",np.around(df_sub_trans.isnull().sum() / df_sub_trans.shape[0],decimals=3))\n",
    "# no missing value! we can use trasactions table to filling members (12.3%), uerlogs (23%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glance at transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort first for easier exploration\n",
    "df_train_trans = df_train_trans.sort_values(by=['msno','trans_date'])\n",
    "df_sub_trans = df_sub_trans.sort_values(by=['msno','trans_date'])\n",
    "df_train_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Date Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort first for easier exploration\n",
    "df_train_trans = df_train_trans.sort_values(by=['msno','trans_date'])\n",
    "df_sub_trans = df_sub_trans.sort_values(by=['msno','trans_date'])\n",
    "\n",
    "# build membership\n",
    "df_train_trans = df_train_trans.eval('membership = mem_expire_date - trans_date')\n",
    "df_sub_trans = df_sub_trans.eval('membership = mem_expire_date - trans_date')\n",
    "\n",
    "# now vectorlized\n",
    "df_train_trans['membership_int'] = df_train_trans.membership / np.timedelta64(1,'D')\n",
    "df_sub_trans['membership_int'] = df_sub_trans.membership / np.timedelta64(1,'D')\n",
    "\n",
    "\n",
    "# glance \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "display_side_by_side(df_train_trans[['membership_int','payment_plan_days']].describe(),\\\n",
    "                    df_sub_trans[['membership_int','payment_plan_days']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "print(\"_\"*15,\" (the training set) df_train_trans \", \"_\"*15)\n",
    "print(''*60)\n",
    "print(\"_\"*30,\" < -30 days\", \"_\"*30)\n",
    "print('is_cancel = 0')\n",
    "min_date = df_train_trans.membership_int.min()\n",
    "con = ((df_train_trans.membership_int.between(min_date, -30)) & (df_train_trans.is_cancel == 0))\n",
    "print(''*60)\n",
    "print(df_train_trans.loc[con,'payment_plan_days'].value_counts().sort_index())\n",
    "are_0_plan_days = df_train_trans.payment_plan_days.isin([0])\n",
    "print(''*60)\n",
    "print(\"is_cancel = 0 , plan_days = 0\" ) \n",
    "print(df_train_trans.loc[con & are_0_plan_days, 'actual_amount_paid'].value_counts().sort_index())\n",
    "# 1449 means 30 days plan , only 1 paid zero, place 30 days also\n",
    "print(''*60)\n",
    "print(\"_\"*30,\" > 450 days\", \"_\"*30)\n",
    "print('is_cancel = 0')\n",
    "print(''*60)\n",
    "print(df_train_trans.loc[df_train_trans.membership > pd.Timedelta('450 days'), 'payment_plan_days'].value_counts().sort_index())\n",
    "print(''*60)\n",
    "print('is_cancel = 0 & plan_days = 0')\n",
    "print(''*60)\n",
    "con = (df_train_trans.payment_plan_days == 0) & (df_train_trans.membership_int > 450)\n",
    "print(df_train_trans.loc[con, 'actual_amount_paid'].value_counts().sort_index())\n",
    "# 100,119,129,149 --> 30days \n",
    "#1599.1788 --> 400 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_time_delta(s):\n",
    "    intervals = {interval:pd.to_timedelta(interval,unit='D') for interval in s.unique()}\n",
    "    return s.map(intervals)\n",
    "\n",
    "# ------------------------------------------negtive min_date ~ -30 days-------------------------------------------------------------\n",
    "# is_cancel = 1\n",
    "  # Clean up by replaceing trans_date \n",
    "min_date = df_train_trans.membership_int.min()\n",
    "con = ((df_train_trans.membership_int.between(min_date, -30)) & (df_train_trans.is_cancel == 1))\n",
    "df_train_trans.loc[con, 'mem_expire_date'] = df_train_trans.loc[con, 'trans_date'] \n",
    "# is_cancel = 0\n",
    "  # Clean up use:\n",
    "    # engineered payment_plan_days_time_delta --> plan_days_td\n",
    "    # payment_plan_days not 0 --> expire = trans + plan_days_td\n",
    "    # payment_plan_days is 0  --> expire = trans + 30 days (explored first with actual_amount_paid)\n",
    "    # the actual_amount_paid could be check on Summarization above\n",
    "\n",
    "df_train_trans['plan_days_td'] = fast_time_delta(df_train_trans.payment_plan_days)\n",
    "\n",
    "\n",
    "  # not_0_plan_days\n",
    "con = ((df_train_trans.membership_int.between(min_date,-30)) & (df_train_trans.is_cancel == 0))\n",
    "not_0_plan_days = ~ (df_train_trans.payment_plan_days.isin([0]))\n",
    "df_train_trans.loc[con & not_0_plan_days,'mem_expire_date'] = df_train_trans.loc[con & not_0_plan_days,'trans_date'] + \\\n",
    "df_train_trans.loc[con & not_0_plan_days,'plan_days_td']\n",
    "\n",
    "  # are_0_plan_days\n",
    "are_0_plan_days = df_train_trans.payment_plan_days.isin([0])\n",
    "df_train_trans.loc[con & are_0_plan_days,'mem_expire_date'] = df_train_trans.loc[con & are_0_plan_days,'trans_date'] + pd.Timedelta('30 days')\n",
    "\n",
    "#  -----------------------------------------positive 450 days ~ max_date-------------------------------------------------------------\n",
    "\n",
    "\n",
    "# is_cancel = 1\n",
    "  # Clean up by replaceing trans_date \n",
    "max_date = df_train_trans.membership_int.max()\n",
    "con = ((df_train_trans.membership_int.between(450, max_date)) & (df_train_trans.is_cancel == 1))\n",
    "df_train_trans.loc[con, 'mem_expire_date'] = df_train_trans.loc[con, 'trans_date'] \n",
    "\n",
    "# is_cancel = 0\n",
    "  # Clean up use:\n",
    "    # payment_plan_days not 0 --> expire = trans + plan_days_td\n",
    "    # payment_plan_days is 0 \n",
    "      # the  30 days case --> expire = trans + 30 days\n",
    "      # the 400 days case --> expire = trnas + 400 days\n",
    "\n",
    "\n",
    "# payment_plan_day not 0\n",
    "con = ((df_train_trans.membership_int.between(450,max_date)) & (df_train_trans.is_cancel == 0))\n",
    "not_0_plan_days = ~ (df_train_trans.payment_plan_days.isin([0]))\n",
    "df_train_trans.loc[con & not_0_plan_days,'mem_expire_date'] = df_train_trans.loc[con & not_0_plan_days,'trans_date'] + \\\n",
    "df_train_trans.loc[con & not_0_plan_days,'plan_days_td']\n",
    "\n",
    "# plan_days = 0 infer by actual_amount_paid\n",
    "# the 30 days case\n",
    "\n",
    "con = (df_train_trans.payment_plan_days == 0) & (df_train_trans.membership_int > 450)\n",
    "\n",
    "paid_30_days = df_train_trans.actual_amount_paid.isin([100,119,129,149])\n",
    "df_train_trans.loc[con & paid_30_days , 'mem_expire_date'] = \\\n",
    "df_train_trans.loc[con & paid_30_days, 'trans_date'] + pd.Timedelta('30 days')\n",
    "\n",
    "# plan_days = 0 infer by actual_amount_paid\n",
    "# the 400 days case\n",
    "paid_400_days = df_train_trans.actual_amount_paid.isin([1599,1788])\n",
    "df_train_trans.loc[con & paid_400_days , 'mem_expire_date'] = \\\n",
    "df_train_trans.loc[con & paid_400_days, 'trans_date'] + pd.Timedelta('400 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summirization\n",
    "print(\"_\"*15,\" (the testing set) df_sub_trans \", \"_\"*15)\n",
    "print(''*60)\n",
    "print(\"_\"*30,\" < -30 days\", \"_\"*30)\n",
    "print('is_cancel = 0')\n",
    "min_date = df_sub_trans.membership_int.min()\n",
    "con = ((df_sub_trans.membership_int.between(min_date, -30)) & (df_sub_trans.is_cancel == 0))\n",
    "print(''*60)\n",
    "print(df_sub_trans.loc[con,'payment_plan_days'].value_counts().sort_index())\n",
    "are_0_plan_days = df_sub_trans.payment_plan_days.isin([0])\n",
    "print(''*60)\n",
    "print(\"is_cancel = 0 , plan_days = 0\" ) \n",
    "print(df_sub_trans.loc[con & are_0_plan_days, 'actual_amount_paid'].value_counts().sort_index())\n",
    "# 1449 means 30 days plan , only 1 paid zero, place 30 days also\n",
    "print(''*60)\n",
    "print(\"_\"*30,\" > 450 days\", \"_\"*30)\n",
    "print('is_cancel = 0')\n",
    "print(''*60)\n",
    "print(df_sub_trans.loc[df_sub_trans.membership > pd.Timedelta('450 days'), 'payment_plan_days'].value_counts().sort_index())\n",
    "print(''*60)\n",
    "print('is_cancel = 0 & plan_days = 0')\n",
    "print(''*60)\n",
    "con = (df_sub_trans.payment_plan_days == 0) & (df_sub_trans.membership_int > 450)\n",
    "print(df_sub_trans.loc[con, 'actual_amount_paid'].value_counts().sort_index())\n",
    "# 100,119,129,149 --> 30days \n",
    "#1599.1788 --> 400 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------negtive min_date ~ -30 days-------------------------------------------------------------\n",
    "# is_cancel = 1\n",
    "  # Clean up by replaceing trans_date \n",
    "min_date = df_sub_trans.membership_int.min()\n",
    "con = ((df_sub_trans.membership_int.between(min_date, -30)) & (df_sub_trans.is_cancel == 1))\n",
    "df_sub_trans.loc[con, 'mem_expire_date'] = df_sub_trans.loc[con, 'trans_date'] \n",
    "# is_cancel = 0\n",
    "  # Clean up use:\n",
    "    # engineered payment_plan_days_time_delta --> plan_days_td\n",
    "    # payment_plan_days not 0 --> expire = trans + plan_days_td\n",
    "    # payment_plan_days is 0  --> expire = trans + 30 days (explored first with actual_amount_paid)\n",
    "    # the actual_amount_paid could be check on Summirization above\n",
    "\n",
    "df_sub_trans['plan_days_td'] = fast_time_delta(df_sub_trans.payment_plan_days)\n",
    "\n",
    "\n",
    "  # not_0_plan_days\n",
    "con = ((df_sub_trans.membership_int.between(min_date,-30)) & (df_sub_trans.is_cancel == 0))\n",
    "not_0_plan_days = ~ (df_sub_trans.payment_plan_days.isin([0]))\n",
    "df_sub_trans.loc[con & not_0_plan_days,'mem_expire_date'] = df_sub_trans.loc[con & not_0_plan_days,'trans_date'] + \\\n",
    "df_sub_trans.loc[con & not_0_plan_days,'plan_days_td']\n",
    "\n",
    "  # are_0_plan_days\n",
    "  # the only one record paid 0 just fill with 30 days treated as NOISE\n",
    "are_0_plan_days = df_sub_trans.payment_plan_days.isin([0])\n",
    "df_sub_trans.loc[con & are_0_plan_days,'mem_expire_date'] = df_sub_trans.loc[con & are_0_plan_days,'trans_date'] + pd.Timedelta('30 days')\n",
    "\n",
    "#  -----------------------------------------positive 450 days ~ max_date-------------------------------------------------------------\n",
    "\n",
    "\n",
    "# is_cancel = 1\n",
    "  # Clean up by replaceing trans_date \n",
    "max_date = df_sub_trans.membership_int.max()\n",
    "con = ((df_sub_trans.membership_int.between(450, max_date)) & (df_sub_trans.is_cancel == 1))\n",
    "df_sub_trans.loc[con, 'mem_expire_date'] = df_sub_trans.loc[con, 'trans_date'] \n",
    "\n",
    "# is_cancel = 0\n",
    "  # Clean up use:\n",
    "    # payment_plan_days not 0 --> expire = trans + plan_days_td\n",
    "    # payment_plan_days is 0 \n",
    "      # the  30 days case --> expire = trans + 30 days\n",
    "      # the 400 days case --> expire = trnas + 400 days\n",
    "\n",
    "\n",
    "# payment_plan_day not 0\n",
    "con = ((df_sub_trans.membership_int.between(450,max_date)) & (df_sub_trans.is_cancel == 0))\n",
    "not_0_plan_days = ~ (df_sub_trans.payment_plan_days.isin([0]))\n",
    "df_sub_trans.loc[con & not_0_plan_days,'mem_expire_date'] = df_sub_trans.loc[con & not_0_plan_days,'trans_date'] + \\\n",
    "df_sub_trans.loc[con & not_0_plan_days,'plan_days_td']\n",
    "\n",
    "# plan_days = 0 infer by actual_amount_paid\n",
    "# the 30 days case\n",
    "\n",
    "con = (df_sub_trans.payment_plan_days == 0) & (df_sub_trans.membership_int > 450)\n",
    "\n",
    "paid_30_days = df_sub_trans.actual_amount_paid.isin([100,119,129,149])\n",
    "df_sub_trans.loc[con & paid_30_days , 'mem_expire_date'] = \\\n",
    "df_sub_trans.loc[con & paid_30_days, 'trans_date'] + pd.Timedelta('30 days')\n",
    "\n",
    "# plan_days = 0 infer by actual_amount_paid\n",
    "# the 400 days case\n",
    "paid_400_days = df_sub_trans.actual_amount_paid.isin([1599])\n",
    "df_sub_trans.loc[con & paid_400_days , 'mem_expire_date'] = \\\n",
    "df_sub_trans.loc[con & paid_400_days, 'trans_date'] + pd.Timedelta('400 days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop leaky predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_trans[df_train_trans.trans_date > np.datetime64('2017-03-01')].shape)\n",
    "print(df_sub_trans[df_sub_trans.trans_date > np.datetime64('2017-04-01')].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trans = df_train_trans[ ~ (  df_train_trans.trans_date > np.datetime64('2017-03-01')  ) ]\n",
    "df_train_trans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "### Last_Last_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_churn(df):\n",
    "  # to avoid mistake by chance, let's sort first\n",
    "  # at the same time, groupby.cumsum has no argument about ascending, so it's necessary\n",
    "  \n",
    "  df = df.sort_values(by=['msno','trans_date'],ascending=True)\n",
    "  df['next_trans_after'] = df['trans_date'].shift(-1) - df['mem_expire_date']\n",
    "  df['the_same_user'] = df['msno'] == df['msno'].shift(-1)\n",
    "  \n",
    "  # 40 days allow more tolerance about churn\n",
    "  df['churn_record'] = (df['next_trans_after'] > pd.Timedelta('30 days')) & (df['the_same_user'] == True)\n",
    "  \n",
    "  # convert boolen to 1 and 0 \n",
    "  df['churn_record'] = df['churn_record']*1\n",
    "  \n",
    "  # sort_by expire_date\n",
    "  df = df.sort_values(by=['msno','mem_expire_date'],ascending=False)\n",
    "\n",
    "  # take out last_last_churn\n",
    "  ft = df.groupby(by=['msno'],as_index=False).nth(1)[['msno','churn_record']]\n",
    "  ft = ft.rename(columns={'churn_record':'last_last_churn'})\n",
    "  \n",
    "  return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_last_last_churn_train = ll_churn(df_train_trans)\n",
    "ft_last_last_churn_sub = ll_churn(df_sub_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train,ft_last_last_churn_train,how='left',on=['msno']);\n",
    "df_sub = pd.merge(df_sub,ft_last_last_churn_train,how='left',on=['msno']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one trans_record does not have last_last_churn information\n",
    "df_train.last_last_churn = df_train.last_last_churn.fillna(-1)\n",
    "df_sub.last_last_churn = df_sub.last_last_churn.fillna(-1)\n",
    "print('_'*30, 'traning set', '_'*30)\n",
    "print(''*60)\n",
    "print(df_train.last_last_churn.value_counts())\n",
    "print('_'*30, 'testing set', '_'*30)\n",
    "print(''*60)\n",
    "print(df_sub.last_last_churn.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ll_churn = df_train.groupby([\"last_last_churn\"])['is_churn'].mean().reset_index().sort_values('is_churn')\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(x='last_last_churn', y='is_churn', data=df_train,ax=ax,order=result_ll_churn['last_last_churn'])\n",
    "ax.axhline(0.049,linestyle='dashed', c='black',alpha = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and prepare features \n",
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "ll_churn_fts = age_fts + ['last_last_churn']\n",
    "exclude_list = ['msno','is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training using features: \\n' ,[fts for fts in ll_churn_fts if fts not in exclude_list])\n",
    "ll_churn_model_xgb, ll_churn_val_score_xgb,\\\n",
    "ll_churn_pred_xgb = model_training_xgb(df_train[ll_churn_fts],df_sub[ll_churn_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_churn_pred_xgb.to_csv(\"/content/datalab/sub_ll_churn_xgb_pred.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/sub_ll_churn_xgb_pred.csv' 'gs://kk_data/submission/sub_ll_churn_xgb_pred.csv';\n",
    "\n",
    "print('age logloss val: ',0.16151)\n",
    "print('age logloss LB: ',0.13161)\n",
    "print('-'*20,' + ll_churn','-'*20)\n",
    "print('ll_churn log_loss val : ',0.15347)\n",
    "print('ll_churn log_loss LB : ',0.12698)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trans_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and merge\n",
    "ft_trans_times_train = df_train_trans[['msno','trans_date']].groupby(by=['msno'],as_index=False).count().\\\n",
    "rename(columns={'trans_date':'trans_times'});\n",
    "ft_trans_times_sub = df_sub_trans[['msno','trans_date']].groupby(by=['msno'],as_index=False).count().\\\n",
    "rename(columns={'trans_date':'trans_times'});\n",
    "df_train = pd.merge(df_train, ft_trans_times_train, how='left', on=['msno'])\n",
    "df_sub = pd.merge(df_sub, ft_trans_times_sub,how='left', on=['msno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_train.append(df_sub)\n",
    "sns.set(font_scale=1.56)\n",
    "# create fig and ax \n",
    "with sns.axes_style('white'):\n",
    "\n",
    "  fig, [ax1, ax2] = plt.subplots(1,2, figsize=(25,10))\n",
    "    \n",
    "  sns.distplot(df_data.loc[(df_data.is_churn == 0), 'trans_times'],kde=False, label='NOT_churn',ax=ax1,norm_hist=True)\n",
    "  sns.distplot(df_data.loc[(df_data.is_churn == 1), 'trans_times'],kde=False, label='churn',ax=ax1,norm_hist=True)\n",
    "  ax1.legend()\n",
    "  ax1.set_yscale('log')\n",
    "\n",
    "  # feature enineering\n",
    "  # New, Trying, OneYear, TwoYear, Stable\n",
    "  max_trans = df_data.trans_times.max()\n",
    "  bins = [0,1,6,12,24,max_trans]\n",
    "\n",
    "  # period = nodes - 1 --> labels = np.arange(len( bins) -1 )\n",
    "  df_data['client_level_code'] = pd.cut(df_data.trans_times, bins, labels= np.arange( len( bins) - 1 ) )\n",
    "  df_data['client_level_code'] = df_data['client_level_code'].astype('int8')\n",
    "\n",
    "  # prepare table (% of each value)\n",
    "  df_percent = pd.DataFrame( df_data['client_level_code'].value_counts().sort_index() / len(df_data) ).round(3) \n",
    "  # barplot of churn rate\n",
    "  sns.barplot(x='client_level_code',y='is_churn',data=df_data,ax=ax2)\n",
    "  ax2.axhline(0.049,linestyle='dashed', c='black',alpha = .3)\n",
    "  table(ax2, df_percent,bbox=[0.80, 0.72, 0.2, 0.27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and prepare features \n",
    "df_train, df_sub = df_data[:len(df_train)], df_data[len(df_train):]\n",
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "client_level_code_fts = ll_churn_fts + ['client_level_code']\n",
    "exclude_list = ['msno','is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training using features: \\n' ,[fts for fts in client_level_code_fts if fts not in exclude_list])\n",
    "client_level_code_model_xgb, client_level_code_val_score_xgb,\\\n",
    "client_level_code_pred_xgb = model_training_xgb(df_train[client_level_code_fts],df_sub[client_level_code_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_level_code_pred_xgb.to_csv(\"/content/datalab/sub_client_level_code_xgb_pred.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/sub_client_level_code_xgb_pred.csv' 'gs://kk_data/submission/sub_client_level_code_xgb_pred.csv';\n",
    "\n",
    "print('last_last_churn logloss val: ',0.15347)\n",
    "print('last_last_churn logloss LB: ',0.12698)\n",
    "print('-'*20,' + client_level_code','-'*20)\n",
    "print('client_level_code log_loss val : ',0.14982)\n",
    "print('client_level_code log_loss LB : ',0.12463)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_renew explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since cumsum does not support ascending = False, we resort our data with mem_expire_date first\n",
    "# need to fix sort_problem\n",
    "df_train_trans = df_train_trans.sort_values(by=['msno'],ascending=False).sort_values(by=['mem_expire_date'],ascending=True)\n",
    "# apply function groupby.cumsum()\n",
    "df_train_trans['auto_renew_so_far'] = df_train_trans.groupby(by=['msno'],as_index=False)['is_auto_renew'].cumsum()\n",
    "# resort with latest expire_date first\n",
    "df_train_trans.sort_values(by=['msno','mem_expire_date'],ascending=False,inplace=True);\n",
    "# since cumcount start at 0, we plus one for adjusting\n",
    "df_train_trans['trans_times_so_far'] = df_train_trans.groupby(by=['msno'],as_index=False)['is_auto_renew'].cumcount(ascending=False) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_result = df_train_trans.groupby(by=['msno'],as_index=False).head(1)\n",
    "agg_result.eval('ratio_auto_renew = auto_renew_so_far / trans_times_so_far', inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "sns.distplot(agg_result.ratio_auto_renew,kde=False,ax=ax)\n",
    "ax.set_yscale('squareroot')\n",
    "print('ratio of All auto_renew:%.3f'%((agg_result.ratio_auto_renew == 1 ).sum() / len(agg_result)))\n",
    "print('ratio of All manual_renew:%.3f'%((agg_result.ratio_auto_renew == 0 ).sum() / len(agg_result)))\n",
    "print('ratio of Mix manual&auto renew:%.3f'%(( (agg_result.ratio_auto_renew < 1) & (agg_result.ratio_auto_renew > 0) ).sum() / len(agg_result)))\n",
    "print('All_auto_renew churn_rate: %.3f' %(agg_result.query('ratio_auto_renew == 1').is_churn.mean()) )\n",
    "print('All_manual_renew churn_rate: %.3f' %(agg_result.query('ratio_auto_renew == 0').is_churn.mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last_auto_renew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# for safe, sorted first\n",
    "df_train_trans = df_train_trans.sort_values(by=['msno','mem_expire_date'],ascending=False)\n",
    "df_sub_trans = df_sub_trans.sort_values(by=['msno','mem_expire_date'],ascending=False)\n",
    "ft_last_auto_renew_train = df_train_trans[['msno','is_auto_renew']].groupby(by=['msno'],as_index=False).head(1)\n",
    "ft_last_auto_renew_sub = df_sub_trans[['msno','is_auto_renew']].groupby(by=['msno'],as_index=False).head(1)\n",
    "# feature rename\n",
    "ft_last_auto_renew_train.rename(columns={'is_auto_renew':'last_auto_renew'}, inplace=True)\n",
    "ft_last_auto_renew_sub.rename(columns={'is_auto_renew':'last_auto_renew'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train,ft_last_auto_renew_train,how='left',on=['msno'])\n",
    "df_sub = pd.merge(df_sub,ft_last_auto_renew_sub,how='left',on=['msno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "  fig, ax = plt.subplots(figsize= (12,8) )\n",
    "# prepare table (% of each value)\n",
    "  df_percent = pd.DataFrame( df_train['last_auto_renew'].value_counts().sort_index() / len(df_train) ).round(3) \n",
    "# barplot with percent table\n",
    "  sns.barplot(x='last_auto_renew',y='is_churn',data=df_train,ax=ax)\n",
    "  ax.axhline(0.049,linestyle='dashed', c='black',alpha = .3)\n",
    "  table(ax, df_percent,bbox=[0.80, 0.72, 0.2, 0.27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and prepare features \n",
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "last_auto_renew_fts = client_level_code_fts + ['last_auto_renew']\n",
    "exclude_list = ['msno','is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training using features: \\n' ,[fts for fts in last_auto_renew_fts if fts not in exclude_list])\n",
    "last_auto_renew_model_xgb, last_auto_renew_val_score_xgb,\\\n",
    "last_auto_renew_pred_xgb = model_training_xgb(df_train[last_auto_renew_fts],df_sub[last_auto_renew_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_auto_renew_pred_xgb.to_csv(\"/content/datalab/sub_last_auto_renew_xgb_pred.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/sub_last_auto_renew_xgb_pred.csv' 'gs://kk_data/submission/sub_last_auto_renew_xgb_pred.csv';\n",
    "\n",
    "print('client_level_code logloss val: ',0.14982)\n",
    "print('client_level_code logloss LB: ',0.12463)\n",
    "print('-'*20,' + last_auto_renew','-'*20)\n",
    "print('last_auto_renew log_loss val : ',0.13419)\n",
    "print('last_auto_renew log_loss LB : ',0.11617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One_month_day_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n query\n",
    "-- one_month_day_listen CTE\n",
    "WITH features AS (\n",
    "                   SELECT  msno AS msno , COUNT(msno) AS one_month_day_listen\n",
    "                   FROM `kkbox-210108.kk_Data.user_logs`\n",
    "                   WHERE date > 20170201 AND date < 20170228 \n",
    "                   GROUP BY msno)\n",
    "SELECT train.msno, features.one_month_day_listen\n",
    "FROM `kkbox-210108.kk_Data.user_label_201703` AS train\n",
    "LEFT JOIN features \n",
    "        ON   train.msno = features.msno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq dryrun -q query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq extract -f csv -H -p 'gs://kk_data/ft_one_month_day_listen.csv' -q query --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq query -n query\n",
    "WITH features AS (\n",
    "                   SELECT  msno AS msno , COUNT(msno) AS one_month_day_listen\n",
    "                   FROM `kkbox-210108.kk_Data.user_logs`\n",
    "                   WHERE date > 20170301 AND date < 20170331 \n",
    "                   GROUP BY msno)\n",
    "SELECT sub.msno, features.one_month_day_listen\n",
    "FROM `kkbox-210108.kk_Data.submission` AS sub\n",
    "LEFT JOIN features \n",
    "        ON   sub.msno = features.msno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq dryrun -q query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bq extract -f csv -H -p 'gs://kk_data/sub_ft_one_month_day_listen.csv' -q query --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/ft_one_month_day_listen.csv -v df_one_month_day_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gcs read --object gs://kk_data/sub_ft_one_month_day_listen.csv -v df_sub_ft_one_month_day_listen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train_one_month_day_listen = pd.read_csv(StringIO(df_one_month_day_listen))\n",
    "ft_sub_one_month_day_listen = pd.read_csv(StringIO(df_sub_ft_one_month_day_listen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, ft_train_one_month_day_listen, how='left',on=['msno'])\n",
    "df_sub = pd.merge(df_sub, ft_sub_one_month_day_listen, how='left',on=['msno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['one_month_day_listen'] = df_train['one_month_day_listen'].fillna(-1)\n",
    "df_sub['one_month_day_listen'] = df_sub['one_month_day_listen'].fillna(-1)\n",
    "fig, ax = plt.subplots( figsize=(20,8) )\n",
    "churn = (df_train['one_month_day_listen'] > 0) & (df_train.is_churn == 1)\n",
    "NOT_churn = (df_train['one_month_day_listen'] > 0) & (df_train.is_churn == 0)\n",
    "bins=25\n",
    "sns.distplot(df_train.loc[churn,'one_month_day_listen'],kde=False,norm_hist=True,label='churn',ax=ax,bins=bins)\n",
    "sns.distplot(df_train.loc[NOT_churn,'one_month_day_listen'],kde=False,norm_hist=True,label='NOT_churn',ax=ax,bins=bins)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "print(df_sub.columns)\n",
    "one_month_day_listen_fts = last_auto_renew_fts + ['one_month_day_listen']\n",
    "exclude_list = ['msno','is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training using features: \\n' ,[fts for fts in one_month_day_listen_fts if fts not in exclude_list])\n",
    "one_month_day_listen_model_xgb, one_month_day_listen_val_score_xgb,\\\n",
    "one_month_day_listen_pred_xgb = model_training_xgb(df_train[one_month_day_listen_fts],df_sub[one_month_day_listen_fts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_day_listen_pred_xgb.to_csv(\"/content/datalab/sub_one_month_day_listen_xgb_pred.csv\", index=False)\n",
    "!gsutil cp '/content/datalab/sub_one_month_day_listen_xgb_pred.csv' 'gs://kk_data/submission/sub_one_month_day_listen_xgb_pred.csv';\n",
    "\n",
    "print('last_auto_renew logloss val: ',0.13419)\n",
    "print('last_auto_renew logloss LB: ',0.11617)\n",
    "print('-'*20,' + one_month_day_listen','-'*20)\n",
    "print('one_month_day_listen log_loss val : ',0.13215)\n",
    "print('one_month_day_listen log_loss LB : ',0.11335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = df_train[one_month_day_listen_fts], df_sub[one_month_day_listen_fts]\n",
    "# splits train and validation set\n",
    "X = training_data.drop(labels=['msno','is_churn'],axis=1)\n",
    "Y = training_data['is_churn']\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,random_state = 2)\n",
    "\n",
    "# model\n",
    "xgb_watchlist = [(X_train, Y_train), (X_val, Y_val)]\n",
    "model = xgb.XGBClassifier(learning_rate=0.08, max_depth=4,n_estimators=300,\\\n",
    "                 subsample=0.5, seed=2,missing=-1)\n",
    "model.fit(X_train, Y_train,eval_set=xgb_watchlist,eval_metric='logloss',\n",
    "            early_stopping_rounds=20,verbose=70)\n",
    "# caculating E_val\n",
    "\n",
    "model_probs = model.predict_proba(X_val)\n",
    "    # [:,1] to show the prob to is_churn = 1\n",
    "model_val_score = log_loss(Y_val,model_probs[:,1])\n",
    "  \n",
    "# predict on testing set\n",
    "model_pred_testing_set = model.predict_proba(testing_data.drop(labels=['msno','is_churn'],axis=1))\n",
    "model_pred_testing_set = model_pred_testing_set[:,1] # take out the prob if is_churn = 1\n",
    "submission = pd.DataFrame({\"msno\": testing_data.msno})\n",
    "submission.insert(1,column='is_churn',value=model_pred_testing_set)\n",
    "  \n",
    "#  return model, model_val_score, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?\n",
    "# X_train = X_train.as_matrix()\n",
    "# X_val = X_val.as_matrix()\n",
    "# Y_train = Y_train.as_matrix()\n",
    "# Y_val = Y_val.as_matrix()\n",
    "print(type(X_train))\n",
    "xgb_watchlist = [(X_train, Y_train), (X_val, Y_val)]\n",
    "model.fit(X_train, Y_train,eval_set=xgb_watchlist,eval_metric='logloss',early_stopping_rounds=20,verbose=70)\n",
    "per = PermutationImportance(model,random_state=1,).fit(X_val,Y_val)\n",
    "# eli5.show_weights(per, feature_names=X_val.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_html = eli5.explain_weights(per)\n",
    "per_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_df = eli5.explain_weights_df(per)\n",
    "\n",
    "feature_names = df_train[one_month_day_listen_fts].drop(labels=['msno','is_churn'],axis=1).columns.tolist()\n",
    "\n",
    "feature_names\n",
    "per_df['feature_name'] = ['last_auto_renew','one_month_day_listen','last_last_churn','registered_via',\n",
    "                         'six_month_day_listen','client_level_code','age_under_26','six_month_satis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(10,6))\n",
    "per_df.plot(kind='barh',y='weight',x='feature_name',ax=ax)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model,importance_type='gain',show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
